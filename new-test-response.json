{"success":true,"data":{"text":"**Parallel Computing Practice Exam**\n\n**Name:** _________________________\n\n**Student ID:** _________________________\n\n\n**Section 1: Multiple Choice (2 points each)**\n\n1.  Which of the following is NOT a primary advantage of parallel computing?\n    a) Increased speed of computation\n    b) Reduced memory requirements\n    c) Ability to solve larger problems\n    d) Improved fault tolerance\n\n    Answer: ________\n\n\n2.  What is Amdahl's Law used to estimate?\n    a) The maximum speedup achievable with parallel processing\n    b) The communication overhead in parallel programs\n    c) The number of processors needed for optimal performance\n    d) The memory bandwidth required for parallel processing\n\n    Answer: ________\n\n\n3.  Which parallel programming model uses a shared memory space accessible by all processors?\n    a) Message Passing Interface (MPI)\n    b) OpenMP\n    c) MapReduce\n    d) CUDA\n\n    Answer: ________\n\n\n4.  A race condition occurs when:\n    a) A program runs faster than expected.\n    b) Multiple threads access and modify the same shared resource concurrently without proper synchronization.\n    c) A deadlock situation arises.\n    d) A process terminates unexpectedly.\n\n    Answer: ________\n\n\n5.  Which of the following is a common technique for handling deadlocks in parallel programming?\n    a) Using semaphores\n    b) Employing mutexes\n    c) Implementing a deadlock prevention strategy\n    d) All of the above\n\n    Answer: ________\n\n\n**Section 2: Short Answer (5 points each)**\n\n6. Briefly explain the difference between coarse-grained and fine-grained parallelism.  Give an example of each.\n\n\nAnswer:\n\n\n7. Describe the concept of load balancing in parallel computing and why it's important.\n\n\nAnswer:\n\n\n**Section 3: Problem Solving (10 points each)**\n\n8.  Consider a program with a sequential portion that takes 20 seconds and a parallelizable portion that takes 60 seconds sequentially.  If we use 4 processors to execute the parallelizable portion, what is the total execution time of the program assuming perfect parallelization? Show your calculations.\n\n\nAnswer:\n\n\n9.  You are designing a parallel program to compute the sum of a large array.  Describe a strategy for dividing the work among multiple processors and explain how you would handle the final aggregation of the partial sums.  Consider potential bottlenecks.\n\n\nAnswer:\n\n\n10. Explain the concept of data parallelism and provide an example of a problem that is well-suited to this approach.\n\n\nAnswer:\n\n\n**Answer Key (For Instructor Use Only):**\n\n1. b\n2. a\n3. b\n4. b\n5. d\n6. (See model answer below)\n7. (See model answer below)\n8. (See model answer below)\n9. (See model answer below)\n10. (See model answer below)\n\n\n**Model Answers (For Instructor Use Only):**\n\n6. **Coarse-grained parallelism** involves dividing a task into relatively large subtasks, each executed by a different processor.  Example:  Processing different images in an image processing pipeline.  **Fine-grained parallelism** involves breaking down a task into many small subtasks, potentially assigning many subtasks to a single processor. Example:  Matrix multiplication where each element calculation is a subtask.\n\n7. **Load balancing** in parallel computing refers to distributing the workload evenly across all processors.  It's important because uneven distribution leads to some processors finishing much earlier than others, creating inefficiency and wasting computational resources.  The overall execution time is limited by the slowest processor.\n\n8. Sequential portion: 20 seconds. Parallelizable portion: 60 seconds / 4 processors = 15 seconds. Total execution time: 20 + 15 = 35 seconds.\n\n9.  Divide the array into chunks, one for each processor. Each processor sums its chunk.  A designated processor (or a reduction operation) collects and sums the partial sums from all processors.  Potential bottleneck: communication overhead in transferring partial sums.  Solutions: Use efficient communication primitives, overlap communication with computation.\n\n10. **Data parallelism** focuses on applying the same operation to multiple data elements concurrently. Example:  Processing each pixel in an image independently to apply a filter (e.g., blurring, edge detection).  Each pixel is processed in parallel.\n","usageMetadata":{"promptTokenCount":60,"candidatesTokenCount":937,"totalTokenCount":997,"promptTokensDetails":[{"modality":"TEXT","tokenCount":60}],"candidatesTokensDetails":[{"modality":"TEXT","tokenCount":937}]},"pdfDownloadUrl":"/api/ai/download-pdf/practice-exam-Parallel-Computing-1750984173377.pdf","pdfGenerated":true}}